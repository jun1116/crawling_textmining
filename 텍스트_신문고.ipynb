{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T12:17:47.109094Z",
     "start_time": "2020-12-06T12:17:46.086859Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T06:26:00.763919Z",
     "start_time": "2020-12-09T06:26:00.663124Z"
    }
   },
   "outputs": [],
   "source": [
    "# 신문고\n",
    "dfsin=pd.read_csv('./test/sinmungo_total.csv')[['text','agency','date','keyword']]\n",
    "dfsin['year'] = pd.to_datetime(dfsin['date']).dt.year\n",
    "print(dfsin.isnull().sum())\n",
    "dfsin = dfsin.dropna(axis=0)\n",
    "# dfsin\n",
    "\n",
    "# # 네이버 다음\n",
    "# # ['title', 'text', 'site', 'source', 'keyword', 'year']\n",
    "# dfnd=pd.read_csv('./test/naver_daum_total.csv')\n",
    "# print(dfnd.isnull().sum())\n",
    "# dfnd = dfnd.dropna(axis=0)\n",
    "# dfnd.isnull().sum()\n",
    "# # dfnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 열을 기준으로 완전히 동일한 중복글의경우 제일 처음 등장한녀석만 살리고, 중복제거 (그래도 어느정도 중복내용존재)\n",
    "# dfsin=dfsin.drop_duplicates(['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 데이터 속 인식 불가단어 미리보기\n",
    "## Kiwi 이용해서 인식불가단어 검출 \n",
    "### (ckonlpy에서 사용자단어 처리할때 사용할 것)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-26T07:48:02.282475Z",
     "start_time": "2020-11-26T07:48:02.135763Z"
    }
   },
   "outputs": [],
   "source": [
    "dfsin['text'].apply(lambda x : x.upper().replace('[^ ㄱ-ㅣ가-힣|0-9|a-zA-Z]', \" \")\n",
    "                   ).to_csv('./test/kiwi_Word_Extract.txt', index=False, header=None, encoding='utf-8' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-26T07:48:11.634648Z",
     "start_time": "2020-11-26T07:48:11.024394Z"
    }
   },
   "outputs": [],
   "source": [
    "from kiwipiepy import Kiwi, Option\n",
    "kiwi = Kiwi(num_workers=0, model_path='./', options=Option.LOAD_DEFAULT_DICTIONARY | Option.INTEGRATE_ALLOMORPH)\n",
    "\n",
    "class ReaderExam:\n",
    "    def __init__(self, filePath):\n",
    "        self.file = open(filePath, encoding='utf-8')\n",
    "    \n",
    "    def read(self, id):\n",
    "        if id == 0: self.file.seek(0)\n",
    "        return self.file.readline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-26T07:48:22.576515Z",
     "start_time": "2020-11-26T07:48:21.022777Z"
    }
   },
   "outputs": [],
   "source": [
    "reader = ReaderExam('./test/kiwi_Word_Extract.txt')\n",
    "kiwiextracted=kiwi.extract_words(reader.read, min_cnt=1000,max_word_len=10, min_score=1.5)\n",
    "kiwiextracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 토큰화, 지역성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 토큰화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 사용자사전 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T06:47:51.118470Z",
     "start_time": "2020-12-01T06:47:51.086654Z"
    }
   },
   "outputs": [],
   "source": [
    "def mkLocationList():\n",
    "    newNouns=set()\n",
    "    with open('./참고파일/지역명/철도역명.txt', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            word=line.replace('\\t','').replace('Noun','').strip('\\n')\n",
    "            if len(word)<2:continue\n",
    "            newNouns.add(word)\n",
    "    with open('./참고파일/지역명/법정동리명.txt', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            word=line.replace('\\t','').replace('Noun','').strip('\\n')\n",
    "            if len(word)<3 : continue\n",
    "            newNouns.add(word)\n",
    "    with open('./참고파일/지역명/시군구명.txt', encoding='utf-8') as f:    \n",
    "        for line in f:\n",
    "            word=line.replace('\\t','').replace('Noun','').strip('\\n')\n",
    "            newNouns.add(word)\n",
    "\n",
    "    return list(newNouns)\n",
    "\n",
    "locations = mkLocationList()\n",
    "\n",
    "def locationToken(tokens):\n",
    "    for token in tokens:\n",
    "        if token in locations:\n",
    "            return 1\n",
    "    return None\n",
    "# locations = mkLocationList()\n",
    "# locations\n",
    "\n",
    "def mkNewWordList():\n",
    "    newWordList=[]\n",
    "    with open('./ckonlpy/사용자단어.txt' ,encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            newWordList.append(line)\n",
    "    return newWordList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T11:07:40.241117Z",
     "start_time": "2020-12-01T11:07:40.013438Z"
    }
   },
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt;okt=Okt()\n",
    "from ckonlpy.utils import load_wordset,load_ngram,load_replace_wordpair\n",
    "from ckonlpy.tag import Twitter\n",
    "from ckonlpy.tag import Postprocessor\n",
    "twitter=Twitter()\n",
    "\n",
    "사용자단어=mkNewWordList()\n",
    "지역명=mkLocationList()\n",
    "twitter.add_dictionary(사용자단어, 'Noun')\n",
    "twitter.add_dictionary(지역명, 'Noun')\n",
    "twitter.add_dictionary(['버스위치'], 'Noun')\n",
    " \n",
    "# ngrams=[(('동탄','신도시'),'Noun'),(('무정','차'),'Noun')]\n",
    "replace = load_replace_wordpair('./ckonlpy/replace.txt')\n",
    "ngrams = load_ngram('./ckonlpy/ngram.txt')\n",
    "stopwords = load_wordset('./ckonlpy/stopwords.txt')\n",
    "passwords = load_wordset('./ckonlpy/passwords.txt')\n",
    "\n",
    "postprocessor = Postprocessor(\n",
    "    base_tagger = twitter, # base tagger\n",
    "    stopwords = stopwords, # 해당 단어 거르기\n",
    "#     passwords = passwords, # 해당 단어만 선택\n",
    "    passtags = {'Noun','Alpha','Number' ,'Adjective' },#, 'Verb'}, # 해당 품사만 선택\n",
    "    replace = replace, # 해당 단어 set 치환\n",
    "    ngrams = ngrams    # 해당 복합 단어 set을 한 단어로 결합\n",
    ")\n",
    "\n",
    "def mkToken(sent):#ngrams에서 ' - ' 로 단어가 붙어나오는걸 띄어쓰기삭제\n",
    "    tmp=re.sub(pattern='[^ ㄱ-ㅣ가-힣|0-9|a-zA-Z]+', repl=' ', string=sent).strip()\n",
    "    return [s.replace(' - ','') for s,v in postprocessor.pos(tmp.upper()) if len(s)>1]\n",
    "\n",
    "# sent='버스노선및 신호등 관련 민원'\n",
    "print(mkToken(sent),'    지역성 : ',locationToken(mkToken(sent)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T13:01:53.748757Z",
     "start_time": "2020-12-01T13:01:53.732762Z"
    }
   },
   "outputs": [],
   "source": [
    "# import kss\n",
    "# from hanspell import spell_checker\n",
    "import re\n",
    "sent='구조변경 차량 버스전용차로 운행 문의(경기남부)'\n",
    "sent = re.sub(pattern='[^ ㄱ-ㅣ가-힣|0-9|a-zA-Z]+', repl=' ', string=sent).strip()\n",
    "# print(spell_checker.check(sent))\n",
    "# print(okt.pos(sent,stem=True))\n",
    "print(kss.split_sentences(sent))\n",
    "print(mkToken(sent))\n",
    "# print(mkToken(sent),'    지역성 : ',locationToken(mkToken(sent)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T12:05:01.402168Z",
     "start_time": "2020-11-29T12:04:11.908199Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 텍스트를 토큰으로 분류\n",
    "# dftest=dfsin.query('year==\"2019\"').reset_index()\n",
    "dfsin['token']=dfsin['text'].apply(lambda x : mkToken(x))\n",
    "dfsin[['text','token']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-27T07:10:43.920301Z",
     "start_time": "2020-11-27T07:10:43.881091Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# text 를 기준으로 간단 중복제거했을때 -> 6098개로, 데이터가 10%만 남게됨\n",
    "dfsin.drop_duplicates('text')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 토큰의 지역분리(경기도 여부)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 (신문고) 데이터 Agency로, 지역성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T11:17:06.097200Z",
     "start_time": "2020-12-01T11:17:06.060241Z"
    }
   },
   "outputs": [],
   "source": [
    "# 관련기관으로 함께 작성해놓은것에서 지역성 text가 얼마나 연관있는지 테스트 -> 거의 ... 효과없음\n",
    "def agencyLot(sent):\n",
    "    locat=['시','군','구','도']\n",
    "    if sent[-1] in locat:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "tmp=dftemp[dftemp['agency'].apply(lambda x : agencyLot(x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T13:31:51.403291Z",
     "start_time": "2020-12-01T13:31:50.482194Z"
    }
   },
   "outputs": [],
   "source": [
    "def newagency(x):\n",
    "    tmp=x['agency']\n",
    "    if agencyLot(tmp)==False : ##시.도 끝나는것 -> 그대로 시, 도 가져가기\n",
    "        return tmp\n",
    "    else : ##시 도 끝나지 않은것 ->  토큰 경기도 검사 -> 경기토큰이면 경기로 , 아니면 그대로 자기기관가져가기\n",
    "        if isTokenGyeonggi(x['token']):\n",
    "            return '경기도'\n",
    "        else:\n",
    "            return tmp\n",
    "\n",
    "\n",
    "dftemp['agency']=dfsin['agency'].apply(lambda x:x.replace('교육청','').split()[0])\n",
    "dftemp['agency']=dftemp[['agency','token']].apply(lambda row : newagency(row) , axis=1)\n",
    "dftemp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T13:34:01.505520Z",
     "start_time": "2020-12-01T13:34:01.169185Z"
    }
   },
   "outputs": [],
   "source": [
    "# dftemp['agency']=dftemp['agency'].apply(lambda x:x.replace('교육청','').split()[0])\n",
    "dftemp.to_csv(\"./지역분류기관으로.csv\",encoding='utf-8')# -> 신문고_지역분리완료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T13:21:52.903696Z",
     "start_time": "2020-12-01T13:21:52.392534Z"
    }
   },
   "outputs": [],
   "source": [
    "#시 도 X\n",
    "tmp[tmp['token'].apply(lambda x:isTokenGyeonggi(x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T12:49:11.402888Z",
     "start_time": "2020-12-01T12:49:11.348653Z"
    }
   },
   "outputs": [],
   "source": [
    "dftemp[dftemp['agency'].apply(lambda x : agencyLot(x)==False)].query(\"agency=='강원도'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 경기도 토큰을 이용하여 경기도에 관한 내용들만 뽑기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T13:05:38.322731Z",
     "start_time": "2020-12-01T13:05:38.312992Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('./ckonlpy/경기_시군구키워드.txt', encoding='utf-8') as f:\n",
    "    tmpset=set()\n",
    "    for line in f:\n",
    "        word=line.replace('\\t','').replace('Noun','').strip('\\n')\n",
    "        if len(word)<2:continue\n",
    "        \n",
    "        words=word.split('.')\n",
    "        if len(words)==1:\n",
    "            tmpset.add(words[0])\n",
    "        else:\n",
    "            for word in words:\n",
    "                tmpset.add(word)\n",
    "#         print(word)\n",
    "gyeonggiToken=list(tmpset)\n",
    "def isAgencyGyeonggi(agency):\n",
    "    if '경기' in agency:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def isTokenGyeonggi(tokens):\n",
    "    for token in tokens:\n",
    "        if token in gyeonggiToken:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def 경기토큰2(tokens):\n",
    "    경기토큰=['운정','운정신도시','하남','하남신도시','교산신도시','교산']\n",
    "    for token in tokens:\n",
    "        if token in 경기토큰:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# dfsin = dfsin[dfsin['token'].apply(lambda x : 경기토큰2(x))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T12:06:10.015550Z",
     "start_time": "2020-11-29T12:06:09.987544Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#####경기도 관련기관으로 선택한것 거르기!\n",
    "dfgyeonggi = dfsin[dfsin['agency'].apply(lambda x: isAgencyGyeonggi(x))]\n",
    "dfgyeonggi \n",
    "# dfsin[ dfsin['token'].apply(lambda x: isTokenGyeonggi(x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T12:06:16.168135Z",
     "start_time": "2020-11-29T12:06:15.466820Z"
    }
   },
   "outputs": [],
   "source": [
    "# 기관경기도 아닌데 토큰에 경기도 내 지역 나타난것\n",
    "dfNotGG = dfsin[dfsin['agency'].apply(lambda x: isAgencyGyeonggi(x)) == False]\n",
    "dfNotGG = dfNotGG[dfNotGG['token'].apply(lambda x: isTokenGyeonggi(x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T12:06:17.658907Z",
     "start_time": "2020-11-29T12:06:17.635910Z"
    }
   },
   "outputs": [],
   "source": [
    "# 경기도를 관련기관으로 선택한것 + 그게아닌데 경기도 내 지역이 토큰검출된것\n",
    "dftotalGG=pd.concat([dfgyeonggi,dfNotGG])#.to_csv('./경기도필터링.csv',sep=':',index=False, encoding='utf-8')\n",
    "dftotalGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 토큰의 수단성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-26T08:41:03.343682Z",
     "start_time": "2020-11-26T08:41:03.325684Z"
    }
   },
   "outputs": [],
   "source": [
    "stationlist=[]\n",
    "with open('./ckonlpy/역명.txt' , 'r',encoding='utf-8') as f:\n",
    "    lines=f.readlines()\n",
    "    for line in lines:\n",
    "        try:\n",
    "            stationlist.append(line.split()[0]+'역')\n",
    "        except:continue\n",
    "stationlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T07:12:16.485714Z",
     "start_time": "2020-12-01T07:12:16.469718Z"
    }
   },
   "outputs": [],
   "source": [
    "waylist=['X',]\n",
    "with open('./ckonlpy/수단.txt' , 'r',encoding='utf-8') as f:\n",
    "    ways=f.readlines()\n",
    "    for way in ways:\n",
    "        waylist.append(way.replace('\\n',''))\n",
    "# waylist\n",
    "stationlist=[]\n",
    "with open('./ckonlpy/역명.txt' , 'r',encoding='utf-8') as f:\n",
    "    lines=f.readlines()\n",
    "    for line in lines:\n",
    "        try:\n",
    "            stationlist.append(line.split()[0]+'역')\n",
    "        except:continue\n",
    "\n",
    "metrolist='인동선,경부선,일산선,경인선,경원선,경춘선,분당선,신분당선,경의선,중앙선,신안산선,\\\n",
    ",안산선,과천선,수인선,경강선,ITX,공항철도,동해선,경전철,의정부경전철,용인경전철,김포골드라인\\\n",
    ",인천공항 자기부상열차,장항선'.split(',')\n",
    "\n",
    "def whichWay(tokens):\n",
    "    waydict={f'{i}':0 for i in waylist}\n",
    "    #급행철도 , 광역급행철도 ,GTXABCD -> GTX\n",
    "    #고속철도 -> KTX\n",
    "    # 호선 -> 지하철\n",
    "    \n",
    "    for token in tokens:\n",
    "        if '호선' in token or '지하철' in token or '경전철' in token or token in metrolist or token in stationlist:#지하철 1 증가\n",
    "            waydict['지하철'] += 1\n",
    "        elif '택시' in token:\n",
    "            waydict['택시']+=1\n",
    "        elif 'M버스' in token or '광역버스' in token or '광역급행버스' in token:\n",
    "            waydict['광역버스']+=1.2\n",
    "        elif '시내버스' in token:\n",
    "            waydict['시내버스']+=1.2\n",
    "        elif '고속버스' in token:\n",
    "            waydict['고속버스']+=1.2\n",
    "        elif '마을버스' in token:\n",
    "            waydict['마을버스']+=1.3\n",
    "        elif '버스' in token or 'BUS' in token or '번' in token or '여객' in token:\n",
    "            waydict['버스']+=1\n",
    "        elif '노선' in token or '배차간격' in token : waydict['버스']+=0.6\n",
    "        elif 'KTX' in token:\n",
    "            waydict['KTX']+=2\n",
    "        elif 'SRT' in token:\n",
    "            waydict['SRT']+=2\n",
    "        elif '트램' in token:\n",
    "            waydict['트램']+=1\n",
    "        elif '광역급행철도' in token or '급행철도' in token or 'GTX' in token:\n",
    "            waydict['GTX'] +=1.5\n",
    "        elif '철도'in token or '도시철도' in token or '철도' == token or '광역철도' in token:\n",
    "            waydict['철도'] +=1\n",
    "    \n",
    "    return sorted(waydict, key=lambda k : waydict[k] , reverse=True)[0]\n",
    "            \n",
    "whichWay(['안산선', '남양', '향남', '노선연장', '요구'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T07:07:13.792276Z",
     "start_time": "2020-12-01T07:07:13.756808Z"
    }
   },
   "outputs": [],
   "source": [
    "# dftotalGG['수단']=dftotalGG['token'].apply(lambda x : whichWay(x))\n",
    "dftotalGG.query(\"수단=='택시'\").head(55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dftotalGG.to_csv('./경기도필터링_수단추가.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 __ 11.19 겸 엑셀토큰합치는과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T06:28:12.013319Z",
     "start_time": "2020-12-09T06:28:11.703865Z"
    }
   },
   "outputs": [],
   "source": [
    "aadfhk=pd.read_csv('./20201205 지역재처리와수단재처리(전국)_겸.csv',encoding='utf-8')\n",
    "aadfhk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T12:18:12.382671Z",
     "start_time": "2020-12-06T12:18:12.152415Z"
    }
   },
   "outputs": [],
   "source": [
    "def rowmerge(x):\n",
    "    tmp=[]\n",
    "    xs=x.dropna()\n",
    "    tmp.extend(xs)\n",
    "    return tmp\n",
    "\n",
    "cols=['수단', '노선', '정류장', '차량', '운전기사', '차고지', '버스전용차로', '광역', '환승', '배차간격', \\\n",
    "      '신설', '개선', '연장', '유지', '추가', '거부', '확정', '불친절', '안내', '오류', '시간', '사고', '요금',\n",
    "       '안전', '친절', '자격요건', '서비스', '코로나', '교통약자']\n",
    "\n",
    "dfhk=pd.read_csv('./지역완료,겸토큰하나로.csv',encoding='utf-8')\n",
    "# dfhk['hktoken']=dfhk[cols].apply(lambda row : rowmerge(row) , axis=1)\n",
    "# dfhk['mtoken']=dfhk[['token','hktoken']].apply(lambda x : hktokenmerge(x) , axis=1)\n",
    "# dfhk=dfhk.drop([cols],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T06:36:53.856453Z",
     "start_time": "2020-12-07T06:36:53.824222Z"
    }
   },
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "import warnings\n",
    "dfhk['token']=dfhk['token'].apply(lambda x : literal_eval(x))\n",
    "dfhk['hktoken']=dfhk['hktoken'].apply(lambda x : literal_eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfsin['token']=dfsin['text'].apply(lambda x : mkToken(x.upper()))\n",
    "dfhk.query(\"year==2020\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftest=dfhk[['text','year','token','hktoken']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T06:37:54.738455Z",
     "start_time": "2020-12-07T06:37:54.394480Z"
    }
   },
   "outputs": [],
   "source": [
    "# csv에서 읽었을때 token 행이 그냥 문자열로 인식 -> 리스트로 바꿔주는 코드\n",
    "from ast import literal_eval\n",
    "def hktokenmerge(x):\n",
    "    tmp=[]\n",
    "    tmp.extend(x.values[0]) #csv에서 리스트 불러오면 str로 그냥 읽혀버려서 그걸 리스트화함\n",
    "    tmp.extend(x.values[1])\n",
    "    return list(set(tmp))\n",
    "dfhk['totaltokens']=dfhk[['token','hktoken']].apply(lambda x : hktokenmerge(x) , axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2.4.1 기존의 신문고 데이터와 합쳐보기 (테스트)\n",
    "####  신문고데이터와 합치면서 중복을 제거해서 진행해본다 \n",
    "####  (완벽한 중복 제거는 거의 불가하니 python의 기본 내장함수 수준으로만 중복제거 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 기존의 신문고 데이터와 합치는부분,  \n",
    "dftmp=pd.merge(dfsin,dftest.drop('token',axis=1),on='text').drop_duplicates('text')\n",
    "dftmp[\"mtoken\"]=dftmp['token']+dftmp['hktoken']\n",
    "# dftmp=dftmp.drop(['token'],axis=1)\n",
    "dftmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 __ 2019년과 2020년으로 분류하여 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=dftest\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2020년 쿼리로 추출하고, 네트워크까지 1123\n",
    "df20=df.query('year==2020')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2019년만 추출해서 네트워크 진행 1124_0019\n",
    "df19 = df.query('year==2019')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T15:07:30.630532Z",
     "start_time": "2020-12-06T15:07:30.606527Z"
    }
   },
   "outputs": [],
   "source": [
    "dfsin['agency'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T15:10:00.820750Z",
     "start_time": "2020-12-06T15:10:00.815130Z"
    }
   },
   "outputs": [],
   "source": [
    "dfsin['agency'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T02:19:19.545205Z",
     "start_time": "2020-12-07T02:19:19.435189Z"
    }
   },
   "outputs": [],
   "source": [
    "## 시.도 별 데이터 개수 뽑기\n",
    "# dfhk.groupby(['agency','year']).describe().to_csv('./2019,2020시도별개수.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-07T06:38:18.371529Z",
     "start_time": "2020-12-07T06:38:18.338224Z"
    }
   },
   "outputs": [],
   "source": [
    "dfhk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T04:13:41.865113Z",
     "start_time": "2020-12-09T04:13:41.852110Z"
    }
   },
   "outputs": [],
   "source": [
    "##겸토큰 하나로 합친거 여기서 dfsin으로 다른이름 지정!\n",
    "\n",
    "dfsin=dfhk.query(\"year=='2020' and agency=='충청북도'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0 워드클라우드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T15:09:35.716725Z",
     "start_time": "2020-12-06T15:09:35.701859Z"
    }
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T04:13:43.453676Z",
     "start_time": "2020-12-09T04:13:43.427679Z"
    }
   },
   "outputs": [],
   "source": [
    "#1차원 리스트로 풀어주기 ->  For Word Counting\n",
    "texts_1d=[]\n",
    "for i in dfsin['totaltokens']:    \n",
    "# for i in dfsin['hktoken']:    \n",
    "#     texts_1d.extend(changeKeyword(i))\n",
    "        texts_1d.extend(i)\n",
    "\n",
    "texts_1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T04:13:46.244750Z",
     "start_time": "2020-12-09T04:13:45.242882Z"
    }
   },
   "outputs": [],
   "source": [
    "# count = Counter(texts)\n",
    "count = Counter(texts_1d)\n",
    "tags = {}\n",
    "for n, c in count.most_common(75):\n",
    "    tags[n] = c\n",
    "\n",
    "wc = WordCloud(font_path='c:/Windows/Fonts/malgun.ttf',\n",
    "               width=800, height=600, scale=2.0, max_font_size=200, background_color=\"#ffffff\")\n",
    "#plt.rc('font', family='Nanum Gothic')\n",
    "\n",
    "gen = wc.generate_from_frequencies(tags)\n",
    "plt.figure()\n",
    "plt.imshow(gen, interpolation='bilinear')\n",
    "plt.show()\n",
    "\n",
    "wc.to_file('./png/신문고_2020_충청북도_토탈토큰.png')\n",
    "\n",
    "#plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.0 Networkx__연관성분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-06T14:43:59.735657Z",
     "start_time": "2020-12-06T14:43:58.665342Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) \n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#\n",
    "## 여기서 쓰인 dataset 이 끝까지 쓰임!!\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T04:13:59.047519Z",
     "start_time": "2020-12-09T04:13:59.035339Z"
    }
   },
   "outputs": [],
   "source": [
    "def changeKeyword(tokens):##대전에서 버스위치가 스위치로 토큰화된부분 처리\n",
    "    tmp=[]\n",
    "    for token in tokens:\n",
    "        tmp.append(token.replace('정보사','정보사이트'))\n",
    "    return tmp\n",
    "dataset=[]\n",
    "for i in dfsin['totaltokens']:\n",
    "# for i in dfsin['hktoken']:    \n",
    "#     dataset.append(i)\n",
    "    dataset.append(changeKeyword(i))\n",
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T03:43:59.170883Z",
     "start_time": "2020-12-09T03:43:59.165889Z"
    }
   },
   "outputs": [],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T04:16:41.606426Z",
     "start_time": "2020-12-09T04:16:41.587430Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from apyori import apriori\n",
    "# # Original 코드\n",
    "# from apyori import apriori\n",
    "# 연관 분석을 수행합니다.\n",
    "# results = list(apriori(dataset,\n",
    "#     min_support=0.06,\n",
    "#     min_confidence=0.05,\n",
    "#     min_lift=1.0,\n",
    "#     max_length=2))\n",
    "# print(results)\n",
    "\n",
    "# # 연관 분석을 수행합니다.\n",
    "results = list(apriori(dataset,\n",
    "    min_support=0.06,\n",
    "    min_confidence=0.8,\n",
    "    min_lift=0.5,\n",
    "    max_length=2))\n",
    "# print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T04:16:57.772957Z",
     "start_time": "2020-12-09T04:16:57.611277Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "# 데이터 프레임 형태로 정리합니다.\n",
    "columns = ['source', 'target', 'support']\n",
    "network_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "### Apriori 알고리즘을, Dataframe으로 변환\n",
    "# 규칙의 조건절을 source, 결과절을 target, 지지도를 support 라는 데이터 프레임의 피처로 변환합니다.\n",
    "for result in results:\n",
    "    if len(result.items) == 2:\n",
    "        items = [x for x in result.items]\n",
    "        row = [items[0], items[1], result.support]\n",
    "        series = pd.Series(row, index=network_df.columns)\n",
    "        network_df = network_df.append(series, ignore_index=True)\n",
    "\n",
    "# print(network_df)\n",
    "\n",
    "#### Node 사이즈를 만들어주기위한 부분\n",
    "dataset1d=[]\n",
    "\n",
    "for i in dataset:\n",
    "    dataset1d.extend(i)\n",
    "\n",
    "count = Counter(dataset1d)\n",
    "\n",
    "node_df = pd.DataFrame(count.items(), columns=['node', 'nodesize'])\n",
    "#원본\n",
    "# node_df = node_df[node_df['nodesize'] >= 5] # 시각화의 편의를 위해 ‘nodesize’ 5 이하는 제거합니다.\n",
    "# 중복ON,전국,통합년도\n",
    "# node_df = node_df[node_df['nodesize'] >= len(dataset)/12] \n",
    "node_df = node_df[node_df['nodesize'] >= 9] \n",
    "\n",
    "# 시각화의 편의를 위해 ‘nodesize’ 5 이하는 제거합니다.\n",
    "# node_df = node_df[node_df['nodesize']>=2] ##데이터 30~40개일때\n",
    "print(node_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T04:16:58.510821Z",
     "start_time": "2020-12-09T04:16:58.234775Z"
    }
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "# import numpy as np\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "\n",
    "# networkx 그래프 객체를 생성합니다.\n",
    "G = nx.Graph()\n",
    "\n",
    "# pr=nx.pageranke(G)\n",
    "\n",
    "# node_df의 키워드 빈도수를 데이터로 하여, 네트워크 그래프의 ‘노드’역할을 하는 원을 생성합니다.\n",
    "for index, row in node_df.iterrows():\n",
    "    G.add_node(row['node'], nodesize=row['nodesize'])\n",
    "\n",
    "# network_df의 연관 분석 데이터를 기반으로, 네트워크 그래프의 ‘관계’역할을 하는 선을 생성합니다.\n",
    "for index, row in network_df.iterrows():\n",
    "    G.add_weighted_edges_from([(row['source'], row['target'], row['support'])])\n",
    "\n",
    "# 그래프 디자인과 관련된 파라미터를 설정합니다. \n",
    "# # original\n",
    "# pos = nx.spring_layout(G, k=0.6, iterations=50)\n",
    "# sizes = [G.nodes[node]['nodesize']*30 for node in G]\n",
    "# #\n",
    "# #원형\n",
    "# pos = nx.spring_layout(G, k=1.7, iterations=50)#2019\n",
    "\n",
    "pos = nx.spring_layout(G, k=3 , iterations=30)#2020\n",
    "##kameda 써보기\n",
    "# pos = nx.kamada_kawai_layout(G)\n",
    "# #\n",
    "# pos=nx.planar_layout(G)\n",
    "# pos=nx.random_layout(G)\n",
    "ncolor2020='#BEEBC8'\n",
    "ncolor2019='#C9D7F5'\n",
    "\n",
    "sizes = [G.nodes[node]['nodesize']*len(dataset)*1 for node in G] #사이즈\n",
    "# sizes = [G.nodes[node]['nodesize']*0.9 for node in G] #사이즈\n",
    "\n",
    "# nx.draw(G, pos=pos, node_size=sizes, node_color=ncolor2019)\n",
    "nx.draw(G, pos=pos, node_size=sizes, node_color=ncolor2020)\n",
    "\n",
    "nx.draw_networkx_labels(G, pos=pos, font_family=\"Malgun Gothic\", font_size=20)\n",
    "\n",
    "# kamada_kawai 써보기!\n",
    "# nx.kamada_kawai_layout(G)\n",
    "\n",
    "# 그래프를 출력합니다.\n",
    "ax = plt.gca()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-09T04:20:38.626113Z",
     "start_time": "2020-12-09T04:20:38.614119Z"
    }
   },
   "outputs": [],
   "source": [
    "## 4.1 네트워크 분석된 것의 키워드에 대한 연관성 추출\n",
    "def mkNetworkWeight(network,keyword):\n",
    "    forkey=pd.DataFrame(network[keyword].items(),columns=['keyword','Weight'])#.sort_values(by=['Weight'])\n",
    "    forkey['Weight']=forkey['keyword'].apply(lambda x: network[keyword][x]['weight'])\n",
    "    return forkey.sort_values(by=['Weight'],ascending=False)\n",
    "\n",
    "# 노선,차량,정류장,운전기사 + 수단들\n",
    "\n",
    "mkNetworkWeight(G,'개선')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-08T11:04:33.445372Z",
     "start_time": "2020-12-08T11:04:33.434370Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "path = 'c:/Windows/Fonts/malgun.ttf'\n",
    "font_name = fm.FontProperties(fname=path, size=50).get_name()\n",
    "print(font_name)\n",
    "plt.rc('font', family=font_name)\n",
    "mpl.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 네트워크 분석된 것의 키워드에 대한 연관성 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-27T12:57:55.897181Z",
     "start_time": "2020-11-27T12:57:55.881926Z"
    }
   },
   "outputs": [],
   "source": [
    "G['노선']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.0.1 LDA 구축\n",
    "#### Mallet LDA vs Gensim LDA -> 정확한 기준이 없이, 둘 중 더 좋은 토픽을 제공하는것을 사용하면됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# data_word=[]\n",
    "# for i in ['hktoken']:\n",
    "#     data_word.append(i)\n",
    "# data_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Mallet__LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T16:54:21.096805Z",
     "start_time": "2020-11-29T16:54:20.968676Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "\n",
    "id2word=corpora.Dictionary(dataset)           ##  Dictionary 로 쓰이는 부분 (DF_DICT)\n",
    "id2word.filter_extremes(no_below = 10)        #   20회 이하로 등장한 단어는 삭제\n",
    "texts = dataset                               ##  Texts로 쓰이는 부분  (tokenized_data)\n",
    "\n",
    "corpus=[id2word.doc2bow(text) for text in texts] ## corpus로 쓰이는 부분 (DF_CORPUS)\n",
    "mallet_path = '/Mallet/bin/mallet' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-27T09:40:05.771310Z",
     "start_time": "2020-11-27T09:39:09.201437Z"
    }
   },
   "outputs": [],
   "source": [
    "ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=10, id2word=id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-27T16:58:00.226702Z",
     "start_time": "2020-11-27T16:58:00.204928Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "import re \n",
    "from gensim.models.ldamodel import LdaModel \n",
    "from gensim.models.callbacks import CoherenceMetric \n",
    "from gensim.models.callbacks import PerplexityMetric \n",
    "import logging \n",
    "import pickle \n",
    "import pyLDAvis.gensim \n",
    "from gensim.models.coherencemodel import CoherenceModel \n",
    "\n",
    "#### Mallet\n",
    "def compute_coherence_values_Mallet(dictionary, corpus, texts, limit, start=4, step=2):\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in tqdm(range(start, limit, step)):\n",
    "#         print(f'{num_topics} 번째 토픽 검사중')\n",
    "        model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "    return model_list, coherence_values\n",
    "\n",
    "#### Gensim LDA\n",
    "def compute_coherence_values(dictionary,corpus,texts,limit,start=5,step=2):\n",
    "    coherence_values=[]\n",
    "    model_list=[]\n",
    "    for num_topics in tqdm(range(start,limit,step)):\n",
    "#         print(f'{num_topics} 번째 토픽 검사중')\n",
    "        model=LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics)\n",
    "        model_list.append(model)\n",
    "        coherencemodel=CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "    return model_list, coherence_values\n",
    "\n",
    "def find_optimal_number_of_topics(ismallet, dictionary, corpus, texts, start=5, limit=21, step=2): \n",
    "    #토픽의 개수를 2~40개사이로 6step에 걸쳐서 나누어 진행\n",
    "    \n",
    "    if ismallet==0:# 일반 Gensim LDA \n",
    "        model_list, coherence_values = compute_coherence_values(dictionary=dictionary, corpus=corpus, texts=texts, start=start, limit=limit, step=step) \n",
    "    else:\n",
    "        model_list, coherence_values = compute_coherence_values_Mallet(dictionary=dictionary, corpus=corpus, texts=texts, start=start, limit=limit, step=step) \n",
    "    \n",
    "    x = range(start, limit, step) \n",
    "    topic_num = 0\n",
    "    count = 0\n",
    "    max_coherence = 0\n",
    "    for m, cv in zip(x, coherence_values):\n",
    "        print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv,5))\n",
    "        coherence = cv\n",
    "        if coherence >= max_coherence:\n",
    "            max_coherence = coherence\n",
    "            topic_num = m\n",
    "            model_list_num = count   \n",
    "        count = count+1\n",
    "    optimal_model = model_list[model_list_num]\n",
    "    model_topics = optimal_model.show_topics(formatted=False)\n",
    "    plt.plot(x, coherence_values) \n",
    "    plt.xlabel(\"Num Topics\") \n",
    "    plt.ylabel(\"Coherence score\") \n",
    "    plt.legend((\"coherence_values\"), loc='best') \n",
    "    plt.show()\n",
    "    return model_list, optimal_model\n",
    "\n",
    "# # Can take a long time to run.\n",
    "# model_list, coherence_values = tqdm(compute_coherence_values(dictionary=id2word, \n",
    "#                                                         corpus=corpus, texts=texts, start=4, limit=21, step=2))\n",
    "\n",
    "# limit=21; start=4; step=2;\n",
    "# x = range(start, limit, step)\n",
    "# topic_num = 0\n",
    "# count = 0\n",
    "# max_coherence = 0\n",
    "# for m, cv in zip(x, coherence_values):\n",
    "#     print(\"Num Topics =\", m, \" has Coherence Value of\", cv)\n",
    "#     coherence = cv\n",
    "#     if coherence >= max_coherence:\n",
    "#         max_coherence = coherence\n",
    "#         topic_num = m\n",
    "#         model_list_num = count   \n",
    "#     count = count+1\n",
    "\n",
    "        \n",
    "# # Select the model and print the topics\n",
    "# optimal_model = model_list[model_list_num]\n",
    "# model_topics = optimal_model.show_topics(formatted=False)\n",
    "# #print(optimal_model.print_topics(num_words=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T17:01:09.773939Z",
     "start_time": "2020-11-29T16:54:27.176201Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# gensim_model_list, gensim_optimal_model = find_optimal_number_of_topics(0,id2word, corpus, texts, start=5, limit=21, step=2)\n",
    "mallet_model_list, mallet_optimal_model = find_optimal_number_of_topics(1,id2word, corpus, texts, start=5, limit=21, step=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T17:01:33.237737Z",
     "start_time": "2020-11-29T17:01:09.774939Z"
    }
   },
   "outputs": [],
   "source": [
    "gensim_model_list, gensim_optimal_model = find_optimal_number_of_topics(0,id2word, corpus, texts, start=5, limit=21, step=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T12:28:46.847167Z",
     "start_time": "2020-11-29T12:28:46.832164Z"
    }
   },
   "outputs": [],
   "source": [
    "# def find_optimal_number_of_topics(ismallet, dictionary, corpus, texts, start=4, limit=21, step=2): \n",
    "#     #토픽의 개수를 2~40개사이로 6step에 걸쳐서 나누어 진행\n",
    "    \n",
    "#     if ismallet==0:# 일반 Gensim LDA \n",
    "#         model_list, coherence_values = compute_coherence_values(dictionary=dictionary, corpus=corpus, texts=texts, start=start, limit=limit, step=step) \n",
    "#     else:\n",
    "#         model_list, coherence_values = compute_coherence_values_Mallet(dictionary=dictionary, corpus=corpus, texts=texts, start=start, limit=limit, step=step) \n",
    "    \n",
    "#     x = range(start, limit, step) \n",
    "#     topic_num = 0\n",
    "#     count = 0\n",
    "#     max_coherence = 0\n",
    "#     for m, cv in zip(x, coherence_values):\n",
    "#         print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv,5))\n",
    "#         coherence = cv\n",
    "#         if coherence >= max_coherence:\n",
    "#             max_coherence = coherence\n",
    "#             topic_num = m\n",
    "#             model_list_num = count   \n",
    "#         count = count+1\n",
    "#     optimal_model = model_list[model_list_num]\n",
    "#     model_topics = optimal_model.show_topics(formatted=False)\n",
    "#     plt.plot(x, coherence_values) \n",
    "#     plt.xlabel(\"Num Topics\") \n",
    "#     plt.ylabel(\"Coherence score\") \n",
    "#     plt.legend((\"coherence_values\"), loc='best') \n",
    "#     plt.show()\n",
    "#     return model_list, optimal_model\n",
    "\n",
    "# model_list, optimal_model = find_optimal_number_of_topics(1,id2word, corpus, texts, start=4, limit=21, step=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 LDA의 토픽 모델링 후 시각화 및 Dataframe으로 뽑아내기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3.1 Mallet의 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T17:01:35.953600Z",
     "start_time": "2020-11-29T17:01:33.238736Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "def mallet_to_lda(optimal_model):\n",
    "    model_gensim = LdaModel(\n",
    "        id2word=optimal_model.id2word, num_topics=optimal_model.num_topics,\n",
    "        alpha=optimal_model.alpha, eta=0, iterations=1000,\n",
    "        gamma_threshold=0.001,\n",
    "        dtype=np.float32\n",
    "    )\n",
    "    model_gensim.sync_state()\n",
    "    model_gensim.state.sstats = optimal_model.wordtopics\n",
    "    return model_gensim\n",
    "\n",
    "model = mallet_to_lda(mallet_model_list[5])\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(model, corpus, id2word)\n",
    "pyLDAvis.save_html(vis, './lda/신문고_Mallet_LDA_20xx_NOT경기topic.html') \n",
    "vis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3.2 Gensim LDA 의 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-27T16:37:08.768679Z",
     "start_time": "2020-11-27T16:37:08.762588Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T17:01:36.864805Z",
     "start_time": "2020-11-29T17:01:35.954600Z"
    }
   },
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(gensim_optimal_model, corpus, id2word)\n",
    "pyLDAvis.save_html(vis, './lda/신문고_LDA_20xx_NOT경기topic.html') \n",
    "vis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### https://projector.tensorflow.org/ 에 시각화하기!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://projector.tensorflow.org/ 에 시각화하기!\n",
    "filename=\"신문고_LDA_20xx\"\n",
    "model.wv.save_word2vec_format(f'./lda/{filename}') # 모델 저장\n",
    "#!python -m gensim.scripts.word2vec2tensor --input f'./lda/{filename}' --output f'./lda/{filename}'\n",
    "!python -m gensim.scripts.word2vec2tensor --input ./lda/신문고_LDA_20xx --output ./lda/신문고_LDA_20xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gensim_optimal_model 구분 잘하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## texts구분잘하기~!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T17:09:52.075735Z",
     "start_time": "2020-11-29T17:09:52.058731Z"
    }
   },
   "outputs": [],
   "source": [
    "#시작 전 세팅\n",
    "# optimal_model = mallet_optimal_model\n",
    "optimal_model = gensim_optimal_model\n",
    "\n",
    "## texts 에 원본 텍스트가 들어가야한다\n",
    "texts = dfsin['text']\n",
    "texts=texts.reset_index()['text']\n",
    "\n",
    "# dataset=dfsin['text']\n",
    "# corpus=corpus\n",
    "# texts=dateset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T17:10:08.099597Z",
     "start_time": "2020-11-29T17:09:53.414807Z"
    }
   },
   "outputs": [],
   "source": [
    "def format_topics_sentences(ldamodel=optimal_model, corpus=corpus, texts=texts):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    #ldamodel[corpus]: lda_model에 corpus를 넣어 각 토픽 당 확률을 알 수 있음\n",
    "    for i, row in tqdm(enumerate(ldamodel[corpus])):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num,topn=10)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "    print(type(sent_topics_df))\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "#     sent_topics_df = pd.concat([sent_topics_df,texts], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=optimal_model, corpus=corpus, texts=texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T17:10:08.115601Z",
     "start_time": "2020-11-29T17:10:08.100597Z"
    }
   },
   "outputs": [],
   "source": [
    "df_topic_sents_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T17:10:08.131604Z",
     "start_time": "2020-11-29T17:10:08.116601Z"
    }
   },
   "outputs": [],
   "source": [
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords#\n",
    "df_dominant_topic.columns = [\n",
    "    'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "# Show\n",
    "df_dominant_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T17:10:08.163612Z",
     "start_time": "2020-11-29T17:10:08.132604Z"
    }
   },
   "outputs": [],
   "source": [
    "# Group top 5 sentences under each topic\n",
    "sent_topics_sorteddf_mallet = pd.DataFrame()\n",
    "\n",
    "sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n",
    "\n",
    "for i, grp in sent_topics_outdf_grpd:\n",
    "    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, \n",
    "                                             grp.sort_values(['Topic_Perc_Contrib'], ascending=[0]).head(1)], \n",
    "                                            axis=0)\n",
    "\n",
    "# Reset Index    \n",
    "sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "topic_counts = df_topic_sents_keywords['Dominant_Topic'].value_counts()\n",
    "topic_counts.sort_index(inplace=True)\n",
    "\n",
    "topic_contribution = round(topic_counts/topic_counts.sum(), 4)\n",
    "topic_contribution\n",
    "\n",
    "lda_inform = pd.concat([sent_topics_sorteddf_mallet, topic_counts, topic_contribution], axis=1)\n",
    "lda_inform.columns=[\"Topic_Num\", \"Topic_Perc_Contrib\", \"Keywords\", \"Text\",\"Num_Documents\", \"Perc_Documents\"]\n",
    "lda_inform = lda_inform[[\"Topic_Num\",\"Keywords\",\"Num_Documents\",\"Perc_Documents\"]]\n",
    "lda_inform\n",
    "# #lda_inform.Topic_Num = lda_inform.Topic_Num.astype(int)\n",
    "# lda_inform['Topic_Num'] =lda_inform['Topic_Num'] +1\n",
    "# lda_inform.Topic_Num = lda_inform.Topic_Num.astype(str)\n",
    "# lda_inform['Topic_Num'] =lda_inform['Topic_Num'].str.split('.').str[0]\n",
    "# df_topic_tweet['Dominant_Topic'] =df_topic_tweet['Dominant_Topic'] +1\n",
    "# df_topic_tweet.Dominant_Topic = df_topic_tweet.Dominant_Topic.astype(str)\n",
    "# df_topic_tweet['Dominant_Topic'] =df_topic_tweet['Dominant_Topic'].str.split('.').str[0]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T17:10:08.195658Z",
     "start_time": "2020-11-29T17:10:08.164611Z"
    }
   },
   "outputs": [],
   "source": [
    "sent_topics_sorteddf = pd.DataFrame()\n",
    "sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n",
    "for i, grp in sent_topics_outdf_grpd:\n",
    "    sent_topics_sorteddf = pd.concat([sent_topics_sorteddf, \n",
    "                                             grp.sort_values(['Topic_Perc_Contrib'], ascending=[0]).head(1)], \n",
    "                                            axis=0)\n",
    "\n",
    "# Reset Index    \n",
    "sent_topics_sorteddf.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Format\n",
    "sent_topics_sorteddf.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Text\"]\n",
    "\n",
    "# Show\n",
    "sent_topics_sorteddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

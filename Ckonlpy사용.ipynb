{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T06:33:07.232441Z",
     "start_time": "2020-12-17T06:33:06.461539Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) \n",
    "path='/Users/hj/Documents/github/crawling_textmining/' \n",
    "filename='test/naver_daum_total.csv'\n",
    "dftemp=pd.read_csv(path+filename,encoding='utf-8')\n",
    "# dftemp['token']=dftemp['token'].apply(lambda x : literal_eval(x))\n",
    "# dftemp['hktoken']=dftemp['hktoken'].apply(lambda x : literal_eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T06:33:08.701620Z",
     "start_time": "2020-12-17T06:33:08.673614Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>site</th>\n",
       "      <th>source</th>\n",
       "      <th>keyword</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ktx노선도 전국</td>\n",
       "      <td>http://ohdaejun.tistory.com/1641</td>\n",
       "      <td>ktx노선도 전국 노선도를 알아봐요 다다음주부터 설날 ktx 예매가 시작되는데요 이...</td>\n",
       "      <td>2019.12.25</td>\n",
       "      <td>daum</td>\n",
       "      <td>blog</td>\n",
       "      <td>KTX</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>속초 KTX 스테이 레지던스 모델하우스</td>\n",
       "      <td>http://apt-land1.tistory.com/40</td>\n",
       "      <td>속초 KTX 스테이 레지던스 모델하우스 최근 현대인들은 삶의 질이 높아지며 여가와 ...</td>\n",
       "      <td>2019.08.18</td>\n",
       "      <td>daum</td>\n",
       "      <td>blog</td>\n",
       "      <td>KTX</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KTX시간표 케이티엑스</td>\n",
       "      <td>http://ohdaejun.tistory.com/1473</td>\n",
       "      <td>하면 제자리라고 말씀드리고 앉으시면 되고요 코레일 홈페이지에서 좌석이랑 KTX시간표...</td>\n",
       "      <td>2019.08.28</td>\n",
       "      <td>daum</td>\n",
       "      <td>blog</td>\n",
       "      <td>KTX</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KTX 노선도 최신판</td>\n",
       "      <td>http://inforssi.tistory.com/97</td>\n",
       "      <td>오늘은 KTX 노선도 최신판을 준비했습니다 가까운 거리는 자차나 지하철 또는 버스를...</td>\n",
       "      <td>2019.11.06</td>\n",
       "      <td>daum</td>\n",
       "      <td>blog</td>\n",
       "      <td>KTX</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KTX 예매  코레일톡</td>\n",
       "      <td>http://travelhistory.tistory.com/60</td>\n",
       "      <td>KTX 예매 코레일톡으로 간단하게 부산행 KTX를 타고 가고있습니다 처음 가보는 부...</td>\n",
       "      <td>2019.11.20</td>\n",
       "      <td>daum</td>\n",
       "      <td>blog</td>\n",
       "      <td>KTX</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69052</th>\n",
       "      <td>인천시 내년 '택시 승차대' 확대내년 4월 '베이형' 설치</td>\n",
       "      <td>https://www.asiatoday.co.kr/view.php?key=20201...</td>\n",
       "      <td>인천시가 택시 승차대 설치를 확대해 비가맹 택시개인택시 등의 수익구조를 개선하고 편...</td>\n",
       "      <td>1일 전</td>\n",
       "      <td>naver</td>\n",
       "      <td>news</td>\n",
       "      <td>택시</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69053</th>\n",
       "      <td>“죽으면 책임질게” 구급차 가로막은 택시기사 징역 2년</td>\n",
       "      <td>https://www.seoul.co.kr/news/newsView.php?id=2...</td>\n",
       "      <td>응급환자를 이송 중이던 구급차를 상대로 고의사고를 내고 사고처리를 요구하며 막아선 ...</td>\n",
       "      <td>2020.10.21.</td>\n",
       "      <td>naver</td>\n",
       "      <td>news</td>\n",
       "      <td>택시</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69054</th>\n",
       "      <td>속보 ‘구급차 막은 택시기사’ 1심서 징역 2년 실형</td>\n",
       "      <td>http://news.kmib.co.kr/article/view.asp?arcid=...</td>\n",
       "      <td>응급환자를 이송 중이던 구급차와 고의로 접촉사고를 낸 혐의로 구속기소된 택시기사가 ...</td>\n",
       "      <td>2020.10.21.</td>\n",
       "      <td>naver</td>\n",
       "      <td>news</td>\n",
       "      <td>택시</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69055</th>\n",
       "      <td>조현준 교수의 '북한 이야기'〈2〉도로 메운 폴크스바겐 택시</td>\n",
       "      <td>https://www.yeongnam.com/web/view.php?key=2020...</td>\n",
       "      <td>호텔 방에 있던 창문을 여니 라선 시내 모습이 눈앞에 펼쳐졌고 폴크스바겐 택시들이 ...</td>\n",
       "      <td>3일 전</td>\n",
       "      <td>naver</td>\n",
       "      <td>news</td>\n",
       "      <td>택시</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69056</th>\n",
       "      <td>\"죽으면 내가 책임진다\" 구급차 막은 택시기사 징역 2년</td>\n",
       "      <td>http://www.segye.com/content/html/2020/10/21/2...</td>\n",
       "      <td>환자 사망 인과관계는 아직 조사 중 단순 접촉사고를 이유로 응급환자가 탄 구급차를 ...</td>\n",
       "      <td>2020.10.21.</td>\n",
       "      <td>naver</td>\n",
       "      <td>news</td>\n",
       "      <td>택시</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69057 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   title  \\\n",
       "0                              ktx노선도 전국   \n",
       "1                  속초 KTX 스테이 레지던스 모델하우스   \n",
       "2                           KTX시간표 케이티엑스   \n",
       "3                            KTX 노선도 최신판   \n",
       "4                           KTX 예매  코레일톡   \n",
       "...                                  ...   \n",
       "69052   인천시 내년 '택시 승차대' 확대내년 4월 '베이형' 설치   \n",
       "69053     “죽으면 책임질게” 구급차 가로막은 택시기사 징역 2년   \n",
       "69054      속보 ‘구급차 막은 택시기사’ 1심서 징역 2년 실형   \n",
       "69055  조현준 교수의 '북한 이야기'〈2〉도로 메운 폴크스바겐 택시   \n",
       "69056    \"죽으면 내가 책임진다\" 구급차 막은 택시기사 징역 2년   \n",
       "\n",
       "                                                    link  \\\n",
       "0                       http://ohdaejun.tistory.com/1641   \n",
       "1                        http://apt-land1.tistory.com/40   \n",
       "2                       http://ohdaejun.tistory.com/1473   \n",
       "3                         http://inforssi.tistory.com/97   \n",
       "4                    http://travelhistory.tistory.com/60   \n",
       "...                                                  ...   \n",
       "69052  https://www.asiatoday.co.kr/view.php?key=20201...   \n",
       "69053  https://www.seoul.co.kr/news/newsView.php?id=2...   \n",
       "69054  http://news.kmib.co.kr/article/view.asp?arcid=...   \n",
       "69055  https://www.yeongnam.com/web/view.php?key=2020...   \n",
       "69056  http://www.segye.com/content/html/2020/10/21/2...   \n",
       "\n",
       "                                                    text         date   site  \\\n",
       "0      ktx노선도 전국 노선도를 알아봐요 다다음주부터 설날 ktx 예매가 시작되는데요 이...   2019.12.25   daum   \n",
       "1      속초 KTX 스테이 레지던스 모델하우스 최근 현대인들은 삶의 질이 높아지며 여가와 ...   2019.08.18   daum   \n",
       "2      하면 제자리라고 말씀드리고 앉으시면 되고요 코레일 홈페이지에서 좌석이랑 KTX시간표...   2019.08.28   daum   \n",
       "3      오늘은 KTX 노선도 최신판을 준비했습니다 가까운 거리는 자차나 지하철 또는 버스를...   2019.11.06   daum   \n",
       "4      KTX 예매 코레일톡으로 간단하게 부산행 KTX를 타고 가고있습니다 처음 가보는 부...   2019.11.20   daum   \n",
       "...                                                  ...          ...    ...   \n",
       "69052  인천시가 택시 승차대 설치를 확대해 비가맹 택시개인택시 등의 수익구조를 개선하고 편...         1일 전  naver   \n",
       "69053  응급환자를 이송 중이던 구급차를 상대로 고의사고를 내고 사고처리를 요구하며 막아선 ...  2020.10.21.  naver   \n",
       "69054  응급환자를 이송 중이던 구급차와 고의로 접촉사고를 낸 혐의로 구속기소된 택시기사가 ...  2020.10.21.  naver   \n",
       "69055  호텔 방에 있던 창문을 여니 라선 시내 모습이 눈앞에 펼쳐졌고 폴크스바겐 택시들이 ...         3일 전  naver   \n",
       "69056  환자 사망 인과관계는 아직 조사 중 단순 접촉사고를 이유로 응급환자가 탄 구급차를 ...  2020.10.21.  naver   \n",
       "\n",
       "      source keyword  year  \n",
       "0       blog     KTX  2019  \n",
       "1       blog     KTX  2019  \n",
       "2       blog     KTX  2019  \n",
       "3       blog     KTX  2019  \n",
       "4       blog     KTX  2019  \n",
       "...      ...     ...   ...  \n",
       "69052   news      택시  2020  \n",
       "69053   news      택시  2020  \n",
       "69054   news      택시  2020  \n",
       "69055   news      택시  2020  \n",
       "69056   news      택시  2020  \n",
       "\n",
       "[69057 rows x 8 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dftemp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T15:08:11.897202Z",
     "start_time": "2020-11-23T15:08:11.502356Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) \n",
    "# 신문고\n",
    "dfsin=pd.read_csv('./test/sinmungo_total.csv')[['text','agency','date','keyword']]\n",
    "dfsin['year'] = pd.to_datetime(dfsin['date']).dt.year\n",
    "dfsin['text'] = dfsin['text'].str.upper()\n",
    "print(dfsin.isnull().sum())\n",
    "dfsin = dfsin.dropna(axis=0)\n",
    "df=dfsin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T16:08:26.263262Z",
     "start_time": "2020-11-19T16:08:26.223271Z"
    }
   },
   "outputs": [],
   "source": [
    "#텍스트 열을 기준으로 완전히 동일한 중복글의경우 제일 처음 등장한녀석만 살리고, 중복제거 (그래도 어느정도 중복내용존재)\n",
    "dfsin=dfsin.drop_duplicates(['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 토큰분류 함수 작성 \n",
    "## 시군구처리 -> \n",
    "## 교통성 , 문제성 이후처리해야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T16:06:20.823640Z",
     "start_time": "2020-11-19T16:06:20.813648Z"
    }
   },
   "outputs": [],
   "source": [
    "# 관련기관으로 함께 작성해놓은것에서 지역성 text가 얼마나 연관있는지 테스트 -> 거의 ... 효과없음\n",
    "def agencyLot(sent):\n",
    "    locat=['시','군','구','도']\n",
    "    if sent[-1] in locat:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "# for i in dfsin[dfsin['agency'].apply(lambda x : agencyLot(x))]['text']:\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T16:06:20.233591Z",
     "start_time": "2020-11-19T16:06:20.191580Z"
    }
   },
   "outputs": [],
   "source": [
    "def mkLocationList():\n",
    "    newNouns=set()\n",
    "    with open('./참고파일/지역명/철도역명.txt', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            word=line.replace('\\t','').replace('Noun','').strip('\\n')\n",
    "            if len(word)<2:continue\n",
    "            newNouns.add(word)\n",
    "    with open('./참고파일/지역명/법정동리명.txt', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            word=line.replace('\\t','').replace('Noun','').strip('\\n')\n",
    "            if len(word)<3 : continue\n",
    "            newNouns.add(word)\n",
    "    with open('./참고파일/지역명/시군구명.txt', encoding='utf-8') as f:    \n",
    "        for line in f:\n",
    "            word=line.replace('\\t','').replace('Noun','').strip('\\n')\n",
    "            newNouns.add(word)\n",
    "    return list(newNouns)\n",
    "locations = mkLocationList()\n",
    "\n",
    "def locationToken(tokens):\n",
    "    for token in tokens:\n",
    "        if token in locations:\n",
    "            return 1\n",
    "    return None\n",
    "# locations = mkLocationList()\n",
    "# locations\n",
    "\n",
    "def mkNewWordList():\n",
    "    newWordList=[]\n",
    "    with open('./ckonlpy/사용자단어.txt' ,encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            newWordList.append(line)\n",
    "    return newWordList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T12:51:53.445096Z",
     "start_time": "2020-11-23T12:51:53.221583Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt;okt=Okt()\n",
    "from ckonlpy.utils import load_wordset,load_ngram,load_replace_wordpair\n",
    "from ckonlpy.tag import Twitter\n",
    "from ckonlpy.tag import Postprocessor\n",
    "twitter=Twitter()\n",
    "\n",
    "사용자단어=mkNewWordList()\n",
    "지역명=mkLocationList()\n",
    "twitter.add_dictionary(사용자단어, 'Noun')\n",
    "twitter.add_dictionary(지역명, 'Noun')\n",
    "# twitter.add_dictionary(['교통비전'], 'Noun')\n",
    " \n",
    "# ngrams=[(('동탄','신도시'),'Noun'),(('무정','차'),'Noun')]\n",
    "replace = load_replace_wordpair('./ckonlpy/replace.txt')\n",
    "ngrams = load_ngram('./ckonlpy/ngram.txt')\n",
    "stopwords = load_wordset('./ckonlpy/stopwords.txt')\n",
    "passwords = load_wordset('./ckonlpy/passwords.txt')\n",
    "\n",
    "postprocessor = Postprocessor(\n",
    "    base_tagger = twitter, # base tagger\n",
    "    stopwords = stopwords, # 해당 단어 거르기\n",
    "#     passwords = passwords, # 해당 단어만 선택\n",
    "    passtags = {'Noun','Alpha','Number'}, #'Adjective'}, # 해당 품사만 선택\n",
    "    replace = replace, # 해당 단어 set 치환\n",
    "    ngrams = ngrams    # 해당 복합 단어 set을 한 단어로 결합\n",
    ")\n",
    "\n",
    "def mkToken(sent):#ngrams에서 ' - ' 로 단어가 붙어나오는걸 띄어쓰기삭제\n",
    "    return [s.replace(' - ','') for s,v in postprocessor.pos(sent.replace('-',' ').upper().strip()) if len(s)>1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# sent=' GTX-D 광역교통망은 2기신도시를 위한 대책입니다 검단신도시 노선확정 필수!'\n",
    "# sent = '마북동 교동마을 대중교통 개선을 위한 용인경전철 노선 포함 요청'\n",
    "sent=' GTX-D 광역교통망은 2기신도시를 위한 대책입니다 검단신도시 노선확정 필수!'\n",
    "# print('before : %s\\n' % twitter.pos(sent))\n",
    "# print('after  : %s\\n' % postprocessor.pos(sent))\n",
    "# print('OKT : %s' % okt.pos(sent, stem=True))\n",
    "print(mkToken(sent),'    지역성 : ',locationToken(mkToken(sent)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 최초 데이터 토큰화부분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T16:09:19.993596Z",
     "start_time": "2020-11-19T16:09:16.519026Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 텍스트를 토큰으로 분류\n",
    "# dftest=dfsin.query('year==\"2019\"').reset_index()\n",
    "dfsin['token']=dfsin['text'].apply(lambda x : mkToken(x.upper()))\n",
    "dfsin[['text','token']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2.1 경기도 토큰으로 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T18:07:54.729002Z",
     "start_time": "2020-11-16T18:07:54.507418Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./ckonlpy/경기_시군구키워드.txt', encoding='utf-8') as f:\n",
    "    tmpset=set()\n",
    "    for line in f:\n",
    "        word=line.replace('\\t','').replace('Noun','').strip('\\n')\n",
    "        if len(word)<3:continue\n",
    "        words=word.split('.')\n",
    "        if len(words)==1:\n",
    "            tmpset.add(words[0])\n",
    "        else:\n",
    "            for word in words:\n",
    "                tmpset.add(word)\n",
    "#         print(word)\n",
    "gyeonggiToken=list(tmpset)\n",
    "def isAgencyGyeonggi(agency):\n",
    "    if '경기' in agency:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def isTokenGyeonggi(tokens):\n",
    "    for token in tokens:\n",
    "        if token in gyeonggiToken:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "#####경기도 거르기!\n",
    "dfgyeonggi = dfsin[dfsin['agency'].apply(lambda x: isAgencyGyeonggi(x))]\n",
    "dfgyeonggi\n",
    "# dfsin[ dfsin['token'].apply(lambda x: isTokenGyeonggi(x)) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T18:07:55.490354Z",
     "start_time": "2020-11-16T18:07:54.730002Z"
    }
   },
   "outputs": [],
   "source": [
    "# 기관경기도 아닌데 토큰에 경기도 나타난것\n",
    "dfNotGG = dfsin[dfsin['agency'].apply(lambda x: isAgencyGyeonggi(x)) == False]\n",
    "dfNotGG = dfNotGG[dfNotGG['token'].apply(lambda x: isTokenGyeonggi(x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T18:07:55.522073Z",
     "start_time": "2020-11-16T18:07:55.490354Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dftotalGG=pd.concat([dfgyeonggi,dfNotGG])#.to_csv('./경기도필터링.csv',sep=':',index=False, encoding='utf-8')\n",
    "dftotalGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T18:07:55.538077Z",
     "start_time": "2020-11-16T18:07:55.523074Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#경기토큰 + 경기지역기관설정 필터된것\n",
    "dftotalGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-13T16:30:00.913350Z",
     "start_time": "2020-11-13T16:29:59.611287Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in dftotalGG['token']:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2.2토큰의 수단성 포함여부"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T17:23:34.752238Z",
     "start_time": "2020-11-16T17:23:34.737235Z"
    }
   },
   "outputs": [],
   "source": [
    "waydict={f'{i}':0 for i in waylist}\n",
    "# waydict['지하철'] += 1\n",
    "waydict\n",
    "sorted(waydict, key=lambda k : waydict[k] , reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T18:16:03.630269Z",
     "start_time": "2020-11-16T18:16:03.618267Z"
    }
   },
   "outputs": [],
   "source": [
    "waylist=['X',]\n",
    "with open('./ckonlpy/수단.txt' , 'r',encoding='utf-8') as f:\n",
    "    ways=f.readlines()\n",
    "    for way in ways:\n",
    "        waylist.append(way.replace('\\n',''))\n",
    "# waylist\n",
    "metrolist='인동선,경부선,일산선,경인선,경원선,경춘선,분당선,신분당선,경의선,중앙선,신안산선,\\\n",
    ",안산선,과천선,수인선,경강선,ITX,공항철도,동해선,경전철,의정부경전철,용인경전철,김포골드라인\\\n",
    ",인천공항 자기부상열차,장항선'.split(',')\n",
    "\n",
    "def whichWay(tokens):\n",
    "    waydict={f'{i}':0 for i in waylist}\n",
    "    #급행철도 , 광역급행철도 ,GTXABCD -> GTX\n",
    "    #고속철도 -> KTX\n",
    "    # 호선 -> 지하철\n",
    "    \n",
    "    for token in tokens:\n",
    "        if '호선' in token or '지하철' in token or '경전철' in token or token in metrolist:#지하철 1 증가\n",
    "            waydict['지하철'] += 1\n",
    "        elif '택시' in token:\n",
    "            waydict['택시']+=1\n",
    "        elif 'M버스' in token or '광역버스' in token or '광역급행버스' in token:\n",
    "            waydict['광역버스']+=1.2\n",
    "        elif '시내버스' in token:\n",
    "            waydict['시내버스']+=1.2\n",
    "        elif '고속버스' in token:\n",
    "            waydict['고속버스']+=1.2\n",
    "        elif '마을버스' in token:\n",
    "            waydict['마을버스']+=1.3\n",
    "        elif '버스' in token or 'BUS' in token or '번' in token or '여객' in token:\n",
    "            waydict['버스']+=1\n",
    "        elif 'KTX' in token:\n",
    "            waydict['KTX']+=1\n",
    "        elif 'SRT' in token:\n",
    "            waydict['SRT']+=1\n",
    "        elif '트램' in token:\n",
    "            waydict['트램']+=1\n",
    "        elif '광역급행철도' in token or '급행철도' in token or 'GTX' in token:\n",
    "            waydict['GTX'] +=1.5\n",
    "        elif '도시철도' in token or '철도' == token or '광역철도' in token:\n",
    "            waydict['철도'] +=1\n",
    "    \n",
    "    return sorted(waydict, key=lambda k : waydict[k] , reverse=True)[0]\n",
    "            \n",
    "whichWay(['안산선', '남양', '향남', '노선연장', '요구'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T18:16:05.951645Z",
     "start_time": "2020-11-16T18:16:05.585919Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dftotalGG['수단']=dftotalGG['token'].apply(lambda x : whichWay(x))\n",
    "dftotalGG.query(\"수단=='X'\").head(55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T18:16:06.661294Z",
     "start_time": "2020-11-16T18:16:06.502704Z"
    }
   },
   "outputs": [],
   "source": [
    "dftotalGG.to_csv('./경기도필터링_수단추가.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3토큰의 지역성 포함여부(전체)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T08:07:09.886911Z",
     "start_time": "2020-11-12T08:06:53.161795Z"
    }
   },
   "outputs": [],
   "source": [
    "#해당 토큰이 지역성을 갖고있는지 -> 1 : 지역성있음 ,  None : 지역성 없음\n",
    "tmp=dfsin['token'].apply(lambda x : locationToken(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T08:09:14.152031Z",
     "start_time": "2020-11-12T08:09:14.139037Z"
    }
   },
   "outputs": [],
   "source": [
    "dfsin['locat']=tmp\n",
    "dftest=dfsin[['text','token']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T08:09:18.583905Z",
     "start_time": "2020-11-12T08:09:18.570059Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfsin[dfsin['locat'].isnull()]['text'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T15:41:08.930588Z",
     "start_time": "2020-11-12T15:41:08.912917Z"
    }
   },
   "outputs": [],
   "source": [
    "sent = '버스정보사이트의 정류장 누락'\n",
    "print(mkToken(sent),'    지역성 : ',locationToken(mkToken(sent)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T08:09:20.174751Z",
     "start_time": "2020-11-12T08:09:19.770808Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 지역명이 안나타났다고 나타난 리스트\n",
    "for i in dfsin[dfsin['locat'].isnull()]['text']:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T08:09:21.230141Z",
     "start_time": "2020-11-12T08:09:21.218751Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfsin[dfsin['locat']==1]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T08:09:24.748071Z",
     "start_time": "2020-11-12T08:09:21.914504Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#지역명이 있다고 나타난 리스트\n",
    "for i in dfsin[dfsin['locat']==1]['text']:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T16:11:18.482357Z",
     "start_time": "2020-11-12T16:11:18.457330Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfsin[dfsin['locat'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T06:48:20.768286Z",
     "start_time": "2020-11-12T06:48:20.754435Z"
    }
   },
   "outputs": [],
   "source": [
    "len(locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T07:37:38.135838Z",
     "start_time": "2020-11-12T07:37:38.132845Z"
    }
   },
   "outputs": [],
   "source": [
    "# \t[다산, 지금, 지구, 버스, 노선, 신설, 계획, 문의]\n",
    "'별내신도시' in locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T16:15:05.804731Z",
     "start_time": "2020-11-11T16:15:05.693034Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./참고파일/지역명/raw/정류장명.txt', encoding='utf-8') as f:\n",
    "    tmpset=set()\n",
    "    for line in f:\n",
    "        word=line.replace('\\t','').replace('Noun','').strip('\\n')\n",
    "        if len(word)<3:continue\n",
    "        words=word.split('.')\n",
    "        if len(words)==1:\n",
    "            tmpset.add(words[0])\n",
    "        else:\n",
    "            for word in words:\n",
    "                tmpset.add(word)\n",
    "#         print(word)\n",
    "tmpset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-11T16:40:47.539543Z",
     "start_time": "2020-11-11T16:40:47.519538Z"
    }
   },
   "outputs": [],
   "source": [
    "len(list(tmpset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T16:15:13.675929Z",
     "start_time": "2020-11-12T16:15:13.649004Z"
    }
   },
   "outputs": [],
   "source": [
    "dftest=pd.DataFrame(dfsin['text'].unique(), columns=['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T16:15:19.031042Z",
     "start_time": "2020-11-12T16:15:19.009270Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dftest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-12T16:16:00.071277Z",
     "start_time": "2020-11-12T16:15:56.769707Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dftest['token'] = dftest['text'].apply(lambda x: mkToken(x))\n",
    "dftest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2020/11/19 엑셀토큰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T15:08:28.143310Z",
     "start_time": "2020-11-23T15:08:26.199049Z"
    }
   },
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "def hktokenmerge(x):\n",
    "    tmp=[]\n",
    "    tmp.extend(literal_eval(x.values[0])) #csv에서 리스트 불러오면 str로 그냥 읽혀버려서 그걸 리스트화함\n",
    "    tmp.extend(x.values[1])\n",
    "    return list(set(tmp))\n",
    "\n",
    "def rowmerge(x):\n",
    "    tmp=[]\n",
    "    xs=x.dropna()\n",
    "    tmp.extend(xs)\n",
    "    return tmp\n",
    "\n",
    "dfhk=pd.read_csv('./HJ-경기도필터링_허겸분류_CSV.csv')\n",
    "cols=['수단', '노선', '정류장', '차량', '운전기사', '차고지', '버스전용차로', '광역', '환승', '배차간격', \\\n",
    "      '신설', '개선', '연장', '유지', '추가', '거부', '확정', '불친절', '안내', '오류', '시간', '사고', '요금',\n",
    "       '안전', '친절', '자격요건', '서비스', '코로나', '교통약자']\n",
    "\n",
    "dfhk['hktoken']=dfhk[cols].apply(lambda row : rowmerge(row) , axis=1)\n",
    "# dfhk['mtoken']=dfhk[['token','hktoken']].apply(lambda x : hktokenmerge(x) , axis=1)\n",
    "# dfhk=dfhk.drop([cols],axis=1)\n",
    "# dfhk.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T15:08:28.205970Z",
     "start_time": "2020-11-23T15:08:28.144311Z"
    }
   },
   "outputs": [],
   "source": [
    "# dfsin['token']=dfsin['text'].apply(lambda x : mkToken(x.upper()))\n",
    "dfhk.query(\"year==2020\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T15:08:29.270039Z",
     "start_time": "2020-11-23T15:08:29.267038Z"
    }
   },
   "outputs": [],
   "source": [
    "dftest=dfhk[['text','year','token']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T15:08:31.719921Z",
     "start_time": "2020-11-23T15:08:29.866045Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def rowmerge(x):\n",
    "    tmp=[]\n",
    "    xs=x.dropna()\n",
    "    tmp.extend(xs)\n",
    "    return tmp\n",
    "dftest['hktoken']=dfhk[cols].apply(lambda row : rowmerge(row) , axis=1)\n",
    "dftest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T15:08:31.751213Z",
     "start_time": "2020-11-23T15:08:31.720921Z"
    }
   },
   "outputs": [],
   "source": [
    "dftest=dftest.dropna(axis=0)\n",
    "dftest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T15:08:32.628060Z",
     "start_time": "2020-11-23T15:08:31.998410Z"
    }
   },
   "outputs": [],
   "source": [
    "# csv에서 읽었을때 token 행이 그냥 문자열로 인식 -> 리스트로 바꿔주는 코드\n",
    "from ast import literal_eval\n",
    "def hktokenmerge(x):\n",
    "    tmp=[]\n",
    "    tmp.extend(literal_eval(x.values[0])) #csv에서 리스트 불러오면 str로 그냥 읽혀버려서 그걸 리스트화함\n",
    "    tmp.extend(x.values[1])\n",
    "    return list(set(tmp))\n",
    "dftest['totaltokens']=dftest[['token','hktoken']].apply(lambda x : hktokenmerge(x) , axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T15:08:33.268785Z",
     "start_time": "2020-11-23T15:08:33.256024Z"
    }
   },
   "outputs": [],
   "source": [
    "dftest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T05:22:00.074840Z",
     "start_time": "2020-11-20T05:22:00.032373Z"
    }
   },
   "outputs": [],
   "source": [
    "dftmp=pd.merge(dfsin,dftest.drop('token',axis=1),on='text').drop_duplicates('text')\n",
    "dftmp[\"mtoken\"]=dftmp['token']+dftmp['hktoken']\n",
    "# dftmp=dftmp.drop(['token'],axis=1)\n",
    "dftmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-20T05:22:44.358818Z",
     "start_time": "2020-11-20T05:22:44.319240Z"
    }
   },
   "outputs": [],
   "source": [
    "dftmp=dftmp.drop(['mtoken'],axis=1)\n",
    "# dftmp.to_csv('./HK토큰포함.csv',encoding='utf-8') #토큰화한걸 합치기위함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11.23 작업 (2019,2020에 대한 처리)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T15:08:36.730913Z",
     "start_time": "2020-11-23T15:08:36.703004Z"
    }
   },
   "outputs": [],
   "source": [
    "df=dftest\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-24T07:46:48.135186Z",
     "start_time": "2020-11-24T07:46:48.117186Z"
    }
   },
   "outputs": [],
   "source": [
    "## 2020년 쿼리로 추출하고, 네트워크까지 1123\n",
    "dftest=df.query('year==2020')\n",
    "dftest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-24T06:52:25.177742Z",
     "start_time": "2020-11-24T06:52:25.161730Z"
    }
   },
   "outputs": [],
   "source": [
    "## 2019년만 추출해서 네트워크 진행 1124_0019\n",
    "dftest = df.query('year==2019')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['인천광역시', '경기도', '울산광역시', '대전광역시', '충청북도', '전라북도', '제주특별자치도',\n",
       "       '경상남도', '충청남도', '서울특별시', '광주광역시', '경상북도', '강원도', '세종특별자치시', '전라남도',\n",
       "       '대구광역시', '부산광역시'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dftemp['agency'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftest=dftemp.query(\"agency=='인천광역시'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 전처리, 토큰분류 이후 과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-24T07:46:56.035866Z",
     "start_time": "2020-11-24T07:46:56.016841Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#1차원 리스트로 풀어주기 ->  For Word Counting\n",
    "texts_1d=[]\n",
    "for i in dftest['hktoken']:    \n",
    "    texts_1d.extend(i)\n",
    "texts_1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-24T07:46:57.438153Z",
     "start_time": "2020-11-24T07:46:56.727669Z"
    }
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "# count = Counter(texts)\n",
    "count = Counter(texts_1d)\n",
    "tags = {}\n",
    "for n, c in count.most_common(100):\n",
    "    tags[n] = c\n",
    "\n",
    "wc = WordCloud(font_path='c:/Windows/Fonts/malgun.ttf',\n",
    "               width=800, height=600, scale=2.0, max_font_size=200, background_color=\"#ffffff\")\n",
    "#plt.rc('font', family='Nanum Gothic')\n",
    "\n",
    "gen = wc.generate_from_frequencies(tags)\n",
    "plt.figure()\n",
    "plt.imshow(gen, interpolation='bilinear')\n",
    "plt.show()\n",
    "\n",
    "wc.to_file('./png/신문고_2019_HK토큰.png')\n",
    "\n",
    "# wc.to_file('./png/신문고_2020_HK토큰.png')\n",
    "\n",
    "#plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-24T07:43:38.029643Z",
     "start_time": "2020-11-24T07:43:37.609282Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.patch.set_facecolor('white')\n",
    "gen = wc.generate_from_frequencies(tags)\n",
    "plt.imshow(gen, interpolation='bilinear')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NetworkX를 통한 네트워크 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T15:22:19.058079Z",
     "start_time": "2020-11-23T15:22:19.055645Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) \n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-24T06:53:05.241827Z",
     "start_time": "2020-11-24T06:53:05.218089Z"
    }
   },
   "outputs": [],
   "source": [
    "dftest#.query('\"수단\"==\"시외버스\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-24T07:03:24.155021Z",
     "start_time": "2020-11-24T07:03:24.090003Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset=[]\n",
    "for i in dftest['hktoken']:\n",
    "    dataset.append(i)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-23T15:32:44.239451Z",
     "start_time": "2020-11-23T15:32:44.230937Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Original 코드\n",
    "# from apyori import apriori\n",
    "# # 연관 분석을 수행합니다.\n",
    "# results = list(apriori(dataset,\n",
    "#     min_support=0.06,\n",
    "#     min_confidence=0.05,\n",
    "#     min_lift=1.0,\n",
    "#     max_length=2))\n",
    "# print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-24T07:03:27.223581Z",
     "start_time": "2020-11-24T07:03:27.184643Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from apyori import apriori\n",
    "# 연관 분석을 수행합니다.\n",
    "results = list(apriori(dataset,\n",
    "    min_support=0.015,\n",
    "    min_confidence=0.03,\n",
    "    min_lift=0.25,\n",
    "    max_length=2))\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-24T07:03:33.921652Z",
     "start_time": "2020-11-24T07:03:33.857386Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 데이터 프레임 형태로 정리합니다.\n",
    "columns = ['source', 'target', 'support']\n",
    "network_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "### Apriori 알고리즘을, Dataframe으로 변환\n",
    "# 규칙의 조건절을 source, 결과절을 target, 지지도를 support 라는 데이터 프레임의 피처로 변환합니다.\n",
    "for result in results:\n",
    "    if len(result.items) == 2:\n",
    "        items = [x for x in result.items]\n",
    "        row = [items[0], items[1], result.support]\n",
    "        series = pd.Series(row, index=network_df.columns)\n",
    "        network_df = network_df.append(series, ignore_index=True)\n",
    "\n",
    "print(network_df)\n",
    "\n",
    "#### Node 사이즈를 만들어주기위한 부분\n",
    "dataset1d=[]\n",
    "\n",
    "for i in dataset:\n",
    "    dataset1d.extend(i)\n",
    "\n",
    "count = Counter(dataset1d)\n",
    "\n",
    "node_df = pd.DataFrame(count.items(), columns=['node', 'nodesize'])\n",
    "node_df = node_df[node_df['nodesize'] >= 5] # 시각화의 편의를 위해 ‘nodesize’ 5 이하는 제거합니다.\n",
    "\n",
    "print(node_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-24T07:03:34.968023Z",
     "start_time": "2020-11-24T07:03:34.946011Z"
    }
   },
   "outputs": [],
   "source": [
    "network_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-24T07:03:37.791391Z",
     "start_time": "2020-11-24T07:03:37.777588Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "dataset1d=[]\n",
    "for i in dataset:\n",
    "    dataset1d.extend(i)\n",
    "\n",
    "count = Counter(dataset1d)\n",
    "\n",
    "node_df = pd.DataFrame(count.items(), columns=['node', 'nodesize'])\n",
    "node_df = node_df[node_df['nodesize'] >= 5]  # 시각화의 편의를 위해 ‘nodesize’ 5 이하는 제거합니다.\n",
    "node_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import warnings\n",
    "import networkx as nx\n",
    "from apyori import apriori\n",
    "import networkx as nx\n",
    "import matplotlib as mpl\n",
    "import matplotlib.font_manager as fm\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) \n",
    "\n",
    "\n",
    "dataset=[]\n",
    "for i in dftest['hktoken']:\n",
    "    dataset.append(i)\n",
    "\n",
    "\n",
    "# 연관 분석.\n",
    "results = list(apriori(dataset,\n",
    "    min_support=0.015,\n",
    "    min_confidence=0.03,\n",
    "    min_lift=0.25,\n",
    "    max_length=2))\n",
    "print(results)\n",
    "\n",
    "\n",
    "# 데이터 프레임 형태로 정리\n",
    "columns = ['source', 'target', 'support']\n",
    "network_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "### Apriori 알고리즘을, Dataframe으로 변환\n",
    "# 규칙의 조건절 source, 결과절 target, 지지도 support 라는 데이터 프레임의 피처로 변환\n",
    "for result in results:\n",
    "    if len(result.items) == 2:\n",
    "        items = [x for x in result.items]\n",
    "        row = [items[0], items[1], result.support]\n",
    "        series = pd.Series(row, index=network_df.columns)\n",
    "        network_df = network_df.append(series, ignore_index=True)\n",
    "\n",
    "print(network_df)\n",
    "\n",
    "#### Node 사이즈를 만들어주기위한 부분\n",
    "dataset1d=[]\n",
    "\n",
    "for i in dataset:\n",
    "    dataset1d.extend(i)\n",
    "\n",
    "count = Counter(dataset1d)\n",
    "\n",
    "node_df = pd.DataFrame(count.items(), columns=['node', 'nodesize'])\n",
    "node_df = node_df[node_df['nodesize'] >= 5] # 시각화의 편의를 위해 ‘nodesize’ 5 이하는 제거합니다.\n",
    "\n",
    "print(node_df.head())\n",
    "\n",
    "dataset1d=[]\n",
    "for i in dataset:\n",
    "    dataset1d.extend(i)\n",
    "\n",
    "count = Counter(dataset1d)\n",
    "\n",
    "node_df = pd.DataFrame(count.items(), columns=['node', 'nodesize'])\n",
    "node_df = node_df[node_df['nodesize'] >= 5]  # 시각화의 편의를 위해 ‘nodesize’ 5 이하는 제거합니다.\n",
    "\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "plt.figure(figsize=(17,17))\n",
    "\n",
    "# networkx 그래프 객체를 생성\n",
    "G = nx.Graph()\n",
    "\n",
    "# node_df의 키워드 빈도수를 데이터로 하여, 네트워크 그래프의 ‘노드’역할을 하는 원 생성\n",
    "for index, row in node_df.iterrows():\n",
    "    G.add_node(row['node'], nodesize=row['nodesize'])\n",
    "\n",
    "# network_df의 연관 분석 데이터를 기반으로, 네트워크 그래프의 ‘관계’역할을 하는 선 생성\n",
    "for index, row in network_df.iterrows():\n",
    "    G.add_weighted_edges_from([(row['source'], row['target'], 2*row['support'])])\n",
    "\n",
    "\n",
    "pos = nx.spring_layout(G, k=2.66 , iterations=60)\n",
    "\n",
    "ncolor2020='#BEEBC8'\n",
    "ncolor2019='#C9D7F5'\n",
    "sizes = [G.nodes[node]['nodesize']*0.76 for node in G] \n",
    "nx.draw(G, pos=pos, node_size=sizes, node_color=ncolor2020)\n",
    "nx.draw_networkx_labels(G, pos=pos, font_family=\"Malgun Gothic\", font_size=26)\n",
    "\n",
    "\n",
    "# 그래프를 출력\n",
    "ax = plt.gca()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-24T07:57:00.716894Z",
     "start_time": "2020-11-24T07:57:00.128664Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "# import numpy as np\n",
    "\n",
    "plt.figure(figsize=(17,17))\n",
    "\n",
    "# networkx 그래프 객체를 생성합니다.\n",
    "G = nx.Graph()\n",
    "\n",
    "# pr=nx.pageranke(G)\n",
    "\n",
    "# node_df의 키워드 빈도수를 데이터로 하여, 네트워크 그래프의 ‘노드’역할을 하는 원을 생성합니다.\n",
    "for index, row in node_df.iterrows():\n",
    "    G.add_node(row['node'], nodesize=row['nodesize'])\n",
    "\n",
    "# network_df의 연관 분석 데이터를 기반으로, 네트워크 그래프의 ‘관계’역할을 하는 선을 생성합니다.\n",
    "for index, row in network_df.iterrows():\n",
    "    G.add_weighted_edges_from([(row['source'], row['target'], 2*row['support'])])\n",
    "\n",
    "# 그래프 디자인과 관련된 파라미터를 설정합니다. \n",
    "# # original\n",
    "# pos = nx.spring_layout(G, k=0.6, iterations=50)\n",
    "# sizes = [G.nodes[node]['nodesize']*30 for node in G]\n",
    "# #\n",
    "# #원형\n",
    "# pos = nx.spring_layout(G, k=1.7, iterations=50)#2019\n",
    "pos = nx.spring_layout(G, k=2.66 , iterations=60)#2020\n",
    "# #\n",
    "# pos=nx.planar_layout(G)\n",
    "# pos=nx.random_layout(G)\n",
    "ncolor2020='#BEEBC8'\n",
    "ncolor2019='#C9D7F5'\n",
    "# sizes = [G.nodes[node]['nodesize']*25 for node in G] #2020\n",
    "sizes = [G.nodes[node]['nodesize']*0.76 for node in G] #2019\n",
    "nx.draw(G, pos=pos, node_size=sizes, node_color=ncolor2020)\n",
    "nx.draw_networkx_labels(G, pos=pos, font_family=\"Malgun Gothic\", font_size=26)\n",
    "\n",
    "\n",
    "\n",
    "# 그래프를 출력합니다.\n",
    "ax = plt.gca()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-24T07:03:41.261988Z",
     "start_time": "2020-11-24T07:03:41.240880Z"
    }
   },
   "outputs": [],
   "source": [
    "def mkNetworkWeight(network,keyword):\n",
    "    forkey=pd.DataFrame(network[keyword].items(),columns=['keyword','Weight'])#.sort_values(by=['Weight'])\n",
    "    forkey['Weight']=forkey['keyword'].apply(lambda x: network[keyword][x]['weight'])\n",
    "    return forkey.sort_values(by=['Weight'],ascending=False)\n",
    "\n",
    "\n",
    "# 노선,차량,정류장,운전기사 + 수단들\n",
    "mkNetworkWeight(G,'마을버스')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-24T05:58:06.233878Z",
     "start_time": "2020-11-24T05:58:06.219605Z"
    }
   },
   "outputs": [],
   "source": [
    "G['노선'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-24T05:59:58.095779Z",
     "start_time": "2020-11-24T05:59:58.078974Z"
    }
   },
   "outputs": [],
   "source": [
    "['지하철','철도','택시','버스','광역버스','마을버스','시내버스','고속버스','KTX','SRT','트램']\n",
    "for i in G.nodes():\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T17:56:22.636176Z",
     "start_time": "2020-11-19T17:56:22.620986Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "path = 'c:/Windows/Fonts/malgun.ttf'\n",
    "font_name = fm.FontProperties(fname=path, size=50).get_name()\n",
    "print(font_name)\n",
    "plt.rc('font', family=font_name)\n",
    "mpl.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# https://happy-obok.tistory.com/5\n",
    "말렛 LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T16:40:42.928645Z",
     "start_time": "2020-11-19T16:40:42.911642Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_word=tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T16:40:44.210477Z",
     "start_time": "2020-11-19T16:40:44.142243Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "id2word=corpora.Dictionary(data_word)\n",
    "id2word.filter_extremes(no_below = 20) #20회 이하로 등장한 단어는 삭제\n",
    "texts = data_word\n",
    "corpus=[id2word.doc2bow(text) for text in texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T16:41:33.655463Z",
     "start_time": "2020-11-19T16:40:45.526556Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=10, id2word=id2word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T16:48:58.820944Z",
     "start_time": "2020-11-19T16:41:33.656448Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=4, step=2):\n",
    "\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=data_word, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values\n",
    "\n",
    "# Can take a long time to run.\n",
    "model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=texts, start=4, limit=21, step=2)\n",
    "\n",
    "limit=21; start=4; step=2;\n",
    "x = range(start, limit, step)\n",
    "topic_num = 0\n",
    "count = 0\n",
    "max_coherence = 0\n",
    "for m, cv in zip(x, coherence_values):\n",
    "    print(\"Num Topics =\", m, \" has Coherence Value of\", cv)\n",
    "    coherence = cv\n",
    "    if coherence >= max_coherence:\n",
    "        max_coherence = coherence\n",
    "        topic_num = m\n",
    "        model_list_num = count   \n",
    "    count = count+1\n",
    "\n",
    "        \n",
    "# Select the model and print the topics\n",
    "optimal_model = model_list[model_list_num]\n",
    "model_topics = optimal_model.show_topics(formatted=False)\n",
    "#print(optimal_model.print_topics(num_words=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# https://github.com/jgtonys/joonggonara_textmining/blob/master/vectorize.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T16:53:44.068481Z",
     "start_time": "2020-11-19T16:53:41.670877Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "\n",
    "#Mallet 사용해보기\n",
    "class MyTokenizer:\n",
    "    def __init__(self, tagger):\n",
    "        self.tagger = tagger\n",
    "    def __call__(self, sent):\n",
    "        pos = self.tagger.pos(sent)\n",
    "        pos = ['{}/{}'.format(word,tag) for word, tag in pos]\n",
    "        return pos\n",
    "token = []\n",
    "nouns = []\n",
    "\n",
    "\n",
    "# filename = '서울/종로구(2019)'\n",
    "\n",
    "# with open('outputs/' + filename + '.txt', 'r') as ft:\n",
    "#     text = ft.readlines()\n",
    "\n",
    "\n",
    "my_tokenizer = MyTokenizer(postprocessor)\n",
    "vectorizer = CountVectorizer(tokenizer = my_tokenizer)\n",
    "X = vectorizer.fit_transform(dftest['text'])\n",
    "\n",
    "Xc = (X.T * X)\n",
    "Xc.setdiag(0)\n",
    "print(Xc.todense())\n",
    "\n",
    "# pandas 를 사용하여 gephi 에서 보여줄 word-word co-occurrence matrix csv 파일을 생성\n",
    "names = vectorizer.get_feature_names() # 엔티티 이름들을 보여준다\n",
    "df = pd.DataFrame(data = Xc.toarray(), columns = names, index = names)\n",
    "df.to_csv('./lda/ForGephi.csv', sep = ',')\n",
    "# mallet_path = '/Mallet/bin/mallet' \n",
    "# ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=10, id2word=id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 젠심모델링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-24T03:12:01.225493Z",
     "start_time": "2020-11-24T03:12:00.865929Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec(sentences = dataset, size = 1500, window = 3, min_count = 5, workers = 5, sg = 1)\n",
    "# model.wv.most_similar(['승차거부'],topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-24T06:04:44.606439Z",
     "start_time": "2020-11-24T06:04:44.599134Z"
    }
   },
   "outputs": [],
   "source": [
    "model.wv.most_similar(['광역버스'],topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T16:23:39.189399Z",
     "start_time": "2020-11-19T16:23:38.158847Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "import re \n",
    "from gensim.models.ldamodel import LdaModel \n",
    "from gensim.models.callbacks import CoherenceMetric \n",
    "from gensim import corpora,models\n",
    "from gensim.models.callbacks import PerplexityMetric \n",
    "import logging \n",
    "import pickle \n",
    "import pyLDAvis.gensim \n",
    "from gensim.models.coherencemodel import CoherenceModel \n",
    "import matplotlib.pyplot as plt\n",
    "def compute_coherence_values(dictionary,corpus,texts,limit,start=2,step=3):\n",
    "    coherence_values=[]\n",
    "    model_list=[]\n",
    "    for num_topics in range(start,limit,step):\n",
    "        model=LdaModel(corpus=corpus, id2word=dictionary,num_topics=num_topics)\n",
    "        model_list.append(model)\n",
    "        coherencemodel=CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "    return model_list, coherence_values\n",
    "\n",
    "def find_optimal_number_of_topics(dictionary, corpus, processed_data): \n",
    "    #토픽의 개수를 2~40개사이로 6step에 걸쳐서 나누어 진행\n",
    "    limit = 40; \n",
    "    start = 2; \n",
    "    step = 6;\n",
    "    model_list, coherence_values = compute_coherence_values(dictionary=dictionary, corpus=corpus, texts=processed_data, start=start, limit=limit, step=step) \n",
    "    x = range(start, limit, step) \n",
    "    plt.plot(x, coherence_values) \n",
    "    plt.xlabel(\"Num Topics\") \n",
    "    plt.ylabel(\"Coherence score\") \n",
    "    plt.legend((\"coherence_values\"), loc='best') \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T16:23:46.049282Z",
     "start_time": "2020-11-19T16:23:45.904790Z"
    }
   },
   "outputs": [],
   "source": [
    "# tokenized_data=[]\n",
    "# for i in dftest['mtoken']:\n",
    "#     tokenized_data.append(i)\n",
    "tokenized_data=dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T16:24:04.839784Z",
     "start_time": "2020-11-19T16:23:49.279647Z"
    }
   },
   "outputs": [],
   "source": [
    "DF_DICT=corpora.Dictionary(tokenized_data)\n",
    "#출현빈도가 적거나 자주등장하는 단어는 제거 -> 중복된것들 정리하는파트\n",
    "DF_DICT.filter_extremes(no_below=10, no_above=0.05)\n",
    "\n",
    "DF_CORPUS=[DF_DICT.doc2bow(text) for text in tokenized_data]\n",
    "print('Number of unique tokens : %d' % len(DF_DICT))\n",
    "print('Number of unique tokens : %d' % len(DF_CORPUS))\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s' ,level=logging.INFO)\n",
    "find_optimal_number_of_topics(DF_DICT,DF_CORPUS,tokenized_data)\n",
    "\n",
    "DF_DICT=corpora.Dictionary(tokenized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T16:25:30.066491Z",
     "start_time": "2020-11-19T16:25:07.707795Z"
    }
   },
   "outputs": [],
   "source": [
    "perplexity_logger = PerplexityMetric(corpus=DF_CORPUS, logger='shell') \n",
    "coherence_logger = CoherenceMetric(corpus=DF_CORPUS, coherence=\"u_mass\", logger='shell')\n",
    "\n",
    "#topic 수 찾았으면 그대로 토픽모델링 시행!\n",
    "lda_model=LdaModel(DF_CORPUS, id2word=DF_DICT, num_topics=20, passes=30, callbacks=[coherence_logger,perplexity_logger])\n",
    "topics = lda_model.print_topics(num_words=8)\n",
    "# for topic in topics:\n",
    "#     print(topic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T16:25:30.081836Z",
     "start_time": "2020-11-19T16:25:30.067491Z"
    }
   },
   "outputs": [],
   "source": [
    "lda_model.show_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T16:25:32.645368Z",
     "start_time": "2020-11-19T16:25:30.082837Z"
    }
   },
   "outputs": [],
   "source": [
    "lda_visualization = pyLDAvis.gensim.prepare(lda_model, DF_CORPUS, DF_DICT, sort_topics=False) \n",
    "# pyLDAvis.save_html(lda_visualization, './lda/신문고_LDA_2019_topic_7.html') \n",
    "pyLDAvis.display(lda_visualization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T16:25:32.660497Z",
     "start_time": "2020-11-19T16:25:32.646672Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_topictable_per_doc(ldamodel, corpus):\n",
    "    topic_table = pd.DataFrame()\n",
    "\n",
    "    # 몇 번째 문서인지를 의미하는 문서 번호와 해당 문서의 토픽 비중을 한 줄씩 꺼내온다.\n",
    "    for i, topic_list in enumerate(ldamodel[corpus]):\n",
    "        doc = topic_list[0] if ldamodel.per_word_topics else topic_list            \n",
    "        doc = sorted(doc, key=lambda x: (x[1]), reverse=True)\n",
    "        # 각 문서에 대해서 비중이 높은 토픽순으로 토픽을 정렬한다.\n",
    "        # EX) 정렬 전 0번 문서 : (2번 토픽, 48.5%), (8번 토픽, 25%), (10번 토픽, 5%), (12번 토픽, 21.5%), \n",
    "        # Ex) 정렬 후 0번 문서 : (2번 토픽, 48.5%), (8번 토픽, 25%), (12번 토픽, 21.5%), (10번 토픽, 5%)\n",
    "        # 48 > 25 > 21 > 5 순으로 정렬이 된 것.\n",
    "\n",
    "        # 모든 문서에 대해서 각각 아래를 수행\n",
    "        for j, (topic_num, prop_topic) in enumerate(doc): #  몇 번 토픽인지와 비중을 나눠서 저장한다.\n",
    "            if j == 0:  # 정렬을 한 상태이므로 가장 앞에 있는 것이 가장 비중이 높은 토픽\n",
    "                topic_table = topic_table.append(pd.Series([int(topic_num), round(prop_topic,4), topic_list]), ignore_index=True)\n",
    "                # 가장 비중이 높은 토픽과, 가장 비중이 높은 토픽의 비중과, 전체 토픽의 비중을 저장한다.\n",
    "            else:\n",
    "                break\n",
    "    return(topic_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T16:25:37.931114Z",
     "start_time": "2020-11-19T16:25:32.661497Z"
    }
   },
   "outputs": [],
   "source": [
    "def format_topics_sentences(ldamodel, corpus, texts):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series(\n",
    "                    [int(topic_num), round(prop_topic, 4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic',\n",
    "                              'Perc_Contribution', 'Topic_Keywords']\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(\n",
    "    ldamodel=lda_model, corpus=DF_CORPUS, texts=dftest['text'])\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = [\n",
    "    'Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "# Show\n",
    "df_dominant_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T16:25:37.947090Z",
     "start_time": "2020-11-19T16:25:37.932114Z"
    }
   },
   "outputs": [],
   "source": [
    "df_dominant_topic.query('Dominant_Topic==6.0')[['Dominant_Topic','Text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-19T16:25:37.978626Z",
     "start_time": "2020-11-19T16:25:37.948089Z"
    }
   },
   "outputs": [],
   "source": [
    "sent_topics_sorteddf = pd.DataFrame()\n",
    "sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n",
    "for i, grp in sent_topics_outdf_grpd:\n",
    "    sent_topics_sorteddf = pd.concat([sent_topics_sorteddf, \n",
    "                                             grp.sort_values(['Perc_Contribution'], ascending=[0]).head(1)], \n",
    "                                            axis=0)\n",
    "\n",
    "# Reset Index    \n",
    "sent_topics_sorteddf.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Format\n",
    "sent_topics_sorteddf.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Text\"]\n",
    "\n",
    "# Show\n",
    "sent_topics_sorteddf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
